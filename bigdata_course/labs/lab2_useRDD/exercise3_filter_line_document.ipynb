{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "762b27f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 10:21:18 WARN Utils: Your hostname, thuan-precision-5560 resolves to a loopback address: 127.0.1.1; using 192.168.1.5 instead (on interface wlp0s20f3)\n",
      "25/09/25 10:21:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/25 10:21:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/25 10:21:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/25 10:21:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created new SparkContext\n",
      "âœ… SparkContext ready - Version: 3.5.6\n",
      "âœ… Application: Lab2_Complete\n"
     ]
    }
   ],
   "source": [
    "# ===== FIXED SETUP FOR JUPYTER =====\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Clear any existing Spark environment variables\n",
    "spark_env_vars = ['SPARK_HOME', 'SPARK_LOCAL_DIRS', 'SPARK_CONF_DIR']\n",
    "for var in spark_env_vars:\n",
    "    if var in os.environ:\n",
    "        del os.environ[var]\n",
    "\n",
    "# Set clean environment\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Import and create SparkContext (only once!)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Check if SparkContext already exists\n",
    "try:\n",
    "    # Try to access existing SparkContext\n",
    "    sc.version\n",
    "    print(\"âœ… Using existing SparkContext\")\n",
    "except:\n",
    "    # Create new SparkContext if none exists\n",
    "    conf = SparkConf().setAppName(\"Lab2_Complete\").setMaster(\"local[*]\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    print(\"âœ… Created new SparkContext\")\n",
    "\n",
    "print(f\"âœ… SparkContext ready - Version: {sc.version}\")\n",
    "print(f\"âœ… Application: {sc.appName}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce93195b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Äá»c file README.md báº±ng Spark\n",
      "âœ… File Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»c thÃ nh RDD\n",
      "âœ… Tá»•ng sá»‘ dÃ²ng: 6\n",
      "âœ… Sá»‘ phÃ¢n vÃ¹ng: 2\n",
      "partitions: [['# Welcome to Apache Spark', 'Apache Spark is a powerful big data processing engine.', 'This lab teaches Spark RDD operations.', 'Spark provides distributed computing capabilities.'], ['Big Data Analytics with Spark framework.', 'Python is great for data science.']]\n",
      "\n",
      "ğŸ“– Táº¥t cáº£ cÃ¡c dÃ²ng trong file:\n",
      "   1: # Welcome to Apache Spark\n",
      "   2: Apache Spark is a powerful big data processing engine.\n",
      "   3: This lab teaches Spark RDD operations.\n",
      "   4: Spark provides distributed computing capabilities.\n",
      "   5: Big Data Analytics with Spark framework.\n",
      "   6: Python is great for data science.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Äá»c file báº±ng Spark\n",
    "print(\"Step 2: Äá»c file README.md báº±ng Spark\")\n",
    "\n",
    "readme_path = \"../../data/README.md\"\n",
    "\n",
    "# Äá»c file thÃ nh RDD\n",
    "lines_rdd = sc.textFile(readme_path)\n",
    "\n",
    "print(f\"âœ… File Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»c thÃ nh RDD\")\n",
    "print(f\"âœ… Tá»•ng sá»‘ dÃ²ng: {lines_rdd.count()}\")\n",
    "print(f\"âœ… Sá»‘ phÃ¢n vÃ¹ng: {lines_rdd.getNumPartitions()}\")\n",
    "\n",
    "print(f\"partitions: {lines_rdd.glom().collect()}\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ táº¥t cáº£ dÃ²ng Ä‘á»ƒ kiá»ƒm tra\n",
    "print(f\"\\nğŸ“– Táº¥t cáº£ cÃ¡c dÃ²ng trong file:\")\n",
    "all_lines = lines_rdd.collect()\n",
    "for i, line in enumerate(all_lines, 1):\n",
    "    print(f\"  {i:2}: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24b771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Lá»c cÃ¡c dÃ²ng cÃ³ chá»©a tá»« 'Spark'\n",
      "âœ… Filter Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng (lazy evaluation)\n",
      "âœ… Sá»‘ dÃ²ng chá»©a 'Spark': 5\n",
      "\n",
      "ğŸ” CÃ¡c dÃ²ng chá»©a tá»« 'Spark':\n",
      "  1: # Welcome to Apache Spark\n",
      "     â†’ # Welcome to Apache **Spark**\n",
      "  2: Apache Spark is a powerful big data processing engine.\n",
      "     â†’ Apache **Spark** is a powerful big data processing engine.\n",
      "  3: This lab teaches Spark RDD operations.\n",
      "     â†’ This lab teaches **Spark** RDD operations.\n",
      "  4: Spark provides distributed computing capabilities.\n",
      "     â†’ **Spark** provides distributed computing capabilities.\n",
      "  5: Big Data Analytics with Spark framework.\n",
      "     â†’ Big Data Analytics with **Spark** framework.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Lá»c cÃ¡c dÃ²ng chá»©a tá»« 'Spark'\n",
    "print(\"\\nStep 3: Lá»c cÃ¡c dÃ²ng cÃ³ chá»©a tá»« 'Spark'\")\n",
    "\n",
    "# Ãp dá»¥ng filter vá»›i lambda function\n",
    "spark_lines_rdd = lines_rdd.filter(lambda line: \"Spark\" in line)\n",
    "\n",
    "print(\"âœ… Filter Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng (lazy evaluation)\")\n",
    "\n",
    "# Äáº¿m sá»‘ dÃ²ng chá»©a 'Spark'\n",
    "spark_count = spark_lines_rdd.count()\n",
    "print(f\"âœ… Sá»‘ dÃ²ng chá»©a 'Spark': {spark_count}\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ cÃ¡c dÃ²ng Ä‘Ã£ lá»c\n",
    "print(f\"\\nğŸ” CÃ¡c dÃ²ng chá»©a tá»« 'Spark':\")\n",
    "filtered_lines = spark_lines_rdd.collect()\n",
    "for i, line in enumerate(filtered_lines, 1):\n",
    "    print(f\"  {i}: {line}\")\n",
    "    # Highlight the word 'Spark' in output\n",
    "    highlighted = line.replace('Spark', '**Spark**')\n",
    "    print(f\"     â†’ {highlighted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0a1e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: LÆ°u cÃ¡c dÃ²ng chá»©a 'Spark' vÃ o file\n",
      "Output path: ../../results/Bai3_spark_lines\n",
      "âœ… ÄÃ£ lÆ°u káº¿t quáº£ vÃ o file\n",
      "\n",
      "ğŸ“ CÃ¡c file Ä‘Æ°á»£c táº¡o trong ../../results/Bai3_spark_lines:\n",
      "  ğŸ“„ ._SUCCESS.crc (8 bytes)\n",
      "  ğŸ“„ .part-00000.crc (12 bytes)\n",
      "  ğŸ“„ .part-00001.crc (12 bytes)\n",
      "  ğŸ“„ _SUCCESS (0 bytes)\n",
      "  ğŸ“„ part-00000 (171 bytes)\n",
      "  ğŸ“„ part-00001 (41 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: LÆ°u káº¿t quáº£ vÃ o file\n",
    "print(\"\\nStep 5: LÆ°u cÃ¡c dÃ²ng chá»©a 'Spark' vÃ o file\")\n",
    "\n",
    "# ÄÆ°á»ng dáº«n output\n",
    "output_path = \"../../results/Bai3_spark_lines\"\n",
    "print(f\"Output path: {output_path}\")\n",
    "\n",
    "# XÃ³a thÆ° má»¥c output cÅ© náº¿u tá»“n táº¡i\n",
    "import shutil\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "    print(\"âœ… ÄÃ£ xÃ³a thÆ° má»¥c output cÅ©\")\n",
    "\n",
    "# Táº¡o thÆ° má»¥c results náº¿u chÆ°a cÃ³\n",
    "os.makedirs(\"../../results\", exist_ok=True)\n",
    "\n",
    "# LÆ°u RDD vÃ o file\n",
    "spark_lines_rdd.saveAsTextFile(output_path)\n",
    "print(\"âœ… ÄÃ£ lÆ°u káº¿t quáº£ vÃ o file\")\n",
    "\n",
    "# Kiá»ƒm tra cÃ¡c file Ä‘Æ°á»£c táº¡o\n",
    "if os.path.exists(output_path):\n",
    "    files = os.listdir(output_path)\n",
    "    print(f\"\\nğŸ“ CÃ¡c file Ä‘Æ°á»£c táº¡o trong {output_path}:\")\n",
    "    for file in sorted(files):\n",
    "        file_path = os.path.join(output_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  ğŸ“„ {file} ({size} bytes)\")\n",
    "else:\n",
    "    print(\"âŒ ThÆ° má»¥c output khÃ´ng Ä‘Æ°á»£c táº¡o\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
