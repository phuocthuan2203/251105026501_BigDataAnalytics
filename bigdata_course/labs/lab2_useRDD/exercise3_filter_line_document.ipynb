{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "762b27f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 10:21:18 WARN Utils: Your hostname, thuan-precision-5560 resolves to a loopback address: 127.0.1.1; using 192.168.1.5 instead (on interface wlp0s20f3)\n",
      "25/09/25 10:21:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/25 10:21:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/25 10:21:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/25 10:21:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created new SparkContext\n",
      "✅ SparkContext ready - Version: 3.5.6\n",
      "✅ Application: Lab2_Complete\n"
     ]
    }
   ],
   "source": [
    "# ===== FIXED SETUP FOR JUPYTER =====\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Clear any existing Spark environment variables\n",
    "spark_env_vars = ['SPARK_HOME', 'SPARK_LOCAL_DIRS', 'SPARK_CONF_DIR']\n",
    "for var in spark_env_vars:\n",
    "    if var in os.environ:\n",
    "        del os.environ[var]\n",
    "\n",
    "# Set clean environment\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Import and create SparkContext (only once!)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Check if SparkContext already exists\n",
    "try:\n",
    "    # Try to access existing SparkContext\n",
    "    sc.version\n",
    "    print(\"✅ Using existing SparkContext\")\n",
    "except:\n",
    "    # Create new SparkContext if none exists\n",
    "    conf = SparkConf().setAppName(\"Lab2_Complete\").setMaster(\"local[*]\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    print(\"✅ Created new SparkContext\")\n",
    "\n",
    "print(f\"✅ SparkContext ready - Version: {sc.version}\")\n",
    "print(f\"✅ Application: {sc.appName}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce93195b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Đọc file README.md bằng Spark\n",
      "✅ File đã được đọc thành RDD\n",
      "✅ Tổng số dòng: 6\n",
      "✅ Số phân vùng: 2\n",
      "partitions: [['# Welcome to Apache Spark', 'Apache Spark is a powerful big data processing engine.', 'This lab teaches Spark RDD operations.', 'Spark provides distributed computing capabilities.'], ['Big Data Analytics with Spark framework.', 'Python is great for data science.']]\n",
      "\n",
      "📖 Tất cả các dòng trong file:\n",
      "   1: # Welcome to Apache Spark\n",
      "   2: Apache Spark is a powerful big data processing engine.\n",
      "   3: This lab teaches Spark RDD operations.\n",
      "   4: Spark provides distributed computing capabilities.\n",
      "   5: Big Data Analytics with Spark framework.\n",
      "   6: Python is great for data science.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Đọc file bằng Spark\n",
    "print(\"Step 2: Đọc file README.md bằng Spark\")\n",
    "\n",
    "readme_path = \"../../data/README.md\"\n",
    "\n",
    "# Đọc file thành RDD\n",
    "lines_rdd = sc.textFile(readme_path)\n",
    "\n",
    "print(f\"✅ File đã được đọc thành RDD\")\n",
    "print(f\"✅ Tổng số dòng: {lines_rdd.count()}\")\n",
    "print(f\"✅ Số phân vùng: {lines_rdd.getNumPartitions()}\")\n",
    "\n",
    "print(f\"partitions: {lines_rdd.glom().collect()}\")\n",
    "\n",
    "# Hiển thị tất cả dòng để kiểm tra\n",
    "print(f\"\\n📖 Tất cả các dòng trong file:\")\n",
    "all_lines = lines_rdd.collect()\n",
    "for i, line in enumerate(all_lines, 1):\n",
    "    print(f\"  {i:2}: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24b771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Lọc các dòng có chứa từ 'Spark'\n",
      "✅ Filter đã được áp dụng (lazy evaluation)\n",
      "✅ Số dòng chứa 'Spark': 5\n",
      "\n",
      "🔍 Các dòng chứa từ 'Spark':\n",
      "  1: # Welcome to Apache Spark\n",
      "     → # Welcome to Apache **Spark**\n",
      "  2: Apache Spark is a powerful big data processing engine.\n",
      "     → Apache **Spark** is a powerful big data processing engine.\n",
      "  3: This lab teaches Spark RDD operations.\n",
      "     → This lab teaches **Spark** RDD operations.\n",
      "  4: Spark provides distributed computing capabilities.\n",
      "     → **Spark** provides distributed computing capabilities.\n",
      "  5: Big Data Analytics with Spark framework.\n",
      "     → Big Data Analytics with **Spark** framework.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Lọc các dòng chứa từ 'Spark'\n",
    "print(\"\\nStep 3: Lọc các dòng có chứa từ 'Spark'\")\n",
    "\n",
    "# Áp dụng filter với lambda function\n",
    "spark_lines_rdd = lines_rdd.filter(lambda line: \"Spark\" in line)\n",
    "\n",
    "print(\"✅ Filter đã được áp dụng (lazy evaluation)\")\n",
    "\n",
    "# Đếm số dòng chứa 'Spark'\n",
    "spark_count = spark_lines_rdd.count()\n",
    "print(f\"✅ Số dòng chứa 'Spark': {spark_count}\")\n",
    "\n",
    "# Hiển thị các dòng đã lọc\n",
    "print(f\"\\n🔍 Các dòng chứa từ 'Spark':\")\n",
    "filtered_lines = spark_lines_rdd.collect()\n",
    "for i, line in enumerate(filtered_lines, 1):\n",
    "    print(f\"  {i}: {line}\")\n",
    "    # Highlight the word 'Spark' in output\n",
    "    highlighted = line.replace('Spark', '**Spark**')\n",
    "    print(f\"     → {highlighted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0a1e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Lưu các dòng chứa 'Spark' vào file\n",
      "Output path: ../../results/Bai3_spark_lines\n",
      "✅ Đã lưu kết quả vào file\n",
      "\n",
      "📁 Các file được tạo trong ../../results/Bai3_spark_lines:\n",
      "  📄 ._SUCCESS.crc (8 bytes)\n",
      "  📄 .part-00000.crc (12 bytes)\n",
      "  📄 .part-00001.crc (12 bytes)\n",
      "  📄 _SUCCESS (0 bytes)\n",
      "  📄 part-00000 (171 bytes)\n",
      "  📄 part-00001 (41 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Lưu kết quả vào file\n",
    "print(\"\\nStep 5: Lưu các dòng chứa 'Spark' vào file\")\n",
    "\n",
    "# Đường dẫn output\n",
    "output_path = \"../../results/Bai3_spark_lines\"\n",
    "print(f\"Output path: {output_path}\")\n",
    "\n",
    "# Xóa thư mục output cũ nếu tồn tại\n",
    "import shutil\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "    print(\"✅ Đã xóa thư mục output cũ\")\n",
    "\n",
    "# Tạo thư mục results nếu chưa có\n",
    "os.makedirs(\"../../results\", exist_ok=True)\n",
    "\n",
    "# Lưu RDD vào file\n",
    "spark_lines_rdd.saveAsTextFile(output_path)\n",
    "print(\"✅ Đã lưu kết quả vào file\")\n",
    "\n",
    "# Kiểm tra các file được tạo\n",
    "if os.path.exists(output_path):\n",
    "    files = os.listdir(output_path)\n",
    "    print(f\"\\n📁 Các file được tạo trong {output_path}:\")\n",
    "    for file in sorted(files):\n",
    "        file_path = os.path.join(output_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  📄 {file} ({size} bytes)\")\n",
    "else:\n",
    "    print(\"❌ Thư mục output không được tạo\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
