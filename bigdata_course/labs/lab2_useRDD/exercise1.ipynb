{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36444e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Environment cleaned\n",
      "üîß Using Python: /home/thuannp4/Development/Python_Projects/bigdata_course/spark_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "# B√†i th·ª±c h√†nh s·ªë 2 - Exercise 1: T·∫°o RDD v√† ph√¢n v√πng d·ªØ li·ªáu\n",
    "# Author: Boss Thu·∫ßn\n",
    "# Clean setup for pip-installed PySpark\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Clear any existing Spark environment variables\n",
    "spark_env_vars = ['SPARK_HOME', 'SPARK_LOCAL_DIRS', 'SPARK_CONF_DIR']\n",
    "for var in spark_env_vars:\n",
    "    if var in os.environ:\n",
    "        del os.environ[var]\n",
    "\n",
    "# Set clean environment for pip-installed PySpark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "print(\"üßπ Environment cleaned\")\n",
    "print(\"üîß Using Python:\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a86cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 10:17:07 WARN Utils: Your hostname, thuan-precision-5560 resolves to a loopback address: 127.0.1.1; using 192.168.1.5 instead (on interface wlp0s20f3)\n",
      "25/09/25 10:17:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/25 10:17:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/25 10:17:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkContext initialized successfully!\n",
      "‚úÖ Spark version: 3.5.6\n",
      "‚úÖ Application name: Lab2_Exercise1\n",
      "‚úÖ Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Create configuration for local mode\n",
    "conf = SparkConf().setAppName(\"Lab2_Exercise1\").setMaster(\"local[*]\")\n",
    "\n",
    "# Create SparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "print(\"‚úÖ SparkContext initialized successfully!\")\n",
    "print(f\"‚úÖ Spark version: {sc.version}\")\n",
    "print(f\"‚úÖ Application name: {sc.appName}\")\n",
    "print(f\"‚úÖ Master: {sc.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccdeffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n",
    "n = rdd1.getNumPartitions()\n",
    "print(n)\n",
    "\n",
    "result = rdd1.glom().collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9315970c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== b) M·ªôt x√¢u k√Ω t·ª± ===\n",
      "number of partitions: 5\n",
      "RDD from string: [['H', 'e'], ['l', 'l'], ['o', ' '], ['S', 'p'], ['a', 'r', 'k']]\n"
     ]
    }
   ],
   "source": [
    "# ===== B√ÄI 1b: X√ÇU K√ù T·ª∞ =====\n",
    "print(\"=== b) M·ªôt x√¢u k√Ω t·ª± ===\")\n",
    "\n",
    "string_data = \"Hello Spark\"\n",
    "rdd2 = sc.parallelize(string_data, 5)\n",
    "print(f\"number of partitions: {rdd2.getNumPartitions()}\")\n",
    "print(f\"RDD from string: {rdd2.glom().collect()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e3c630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== c) M·ªôt danh s√°ch c√°c x√¢u k√Ω t·ª± ===\n",
      "S·ªë ph√¢n v√πng: 3\n",
      "Ph√¢n v√πng chi ti·∫øt: [['Apache'], ['Spark', 'Big'], ['Data', 'Analytics']]\n"
     ]
    }
   ],
   "source": [
    "# ===== B√ÄI 1c: DANH S√ÅCH C√ÅC X√ÇU K√ù T·ª∞ =====\n",
    "print(\"=== c) M·ªôt danh s√°ch c√°c x√¢u k√Ω t·ª± ===\")\n",
    "\n",
    "string_list = [\"Apache\", \"Spark\", \"Big\", \"Data\", \"Analytics\"]\n",
    "rdd3 = sc.parallelize(string_list, 3)\n",
    "print(f\"S·ªë ph√¢n v√πng: {rdd3.getNumPartitions()}\")\n",
    "print(f\"Ph√¢n v√πng chi ti·∫øt: {rdd3.glom().collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce9296d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detaied partitions: [['hello'], ['goodbye', 'thank you'], ['please'], ['sorry', 'yes']]\n",
      "Detaied partitions after converted: [[('hello', 'xin ch√†o')], [('goodbye', 't·∫°m bi·ªát'), ('thank you', 'c·∫£m ∆°n')], [('please', 'xin')], [('sorry', 'xin l·ªói'), ('yes', 'v√¢ng')]]\n"
     ]
    }
   ],
   "source": [
    "data_dict = {\n",
    "    'hello': 'xin ch√†o',\n",
    "    'goodbye': 't·∫°m bi·ªát',\n",
    "    'thank you': 'c·∫£m ∆°n',\n",
    "    'please': 'xin',\n",
    "    'sorry': 'xin l·ªói',\n",
    "    'yes': 'v√¢ng'\n",
    "}\n",
    "\n",
    "rdd4 = sc.parallelize(data_dict, 4)\n",
    "print(f\"Detaied partitions: {rdd4.glom().collect()}\")\n",
    "\n",
    "rdd4 = sc.parallelize(data_dict.items(), 4) # convert dict to list or tuple for leveraging full PySpark support\n",
    "print(f\"Detaied partitions after converted: {rdd4.glom().collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b2f9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detaied partitions: [[42], ['Hello World', [1, 2, 3]], [(4, 5, 6), {'key': 'value'}], [3.14, True]]\n"
     ]
    }
   ],
   "source": [
    "mixed_data = [\n",
    "    42,                           # s·ªë nguy√™n\n",
    "    \"Hello World\",               # x√¢u k√Ω t·ª±\n",
    "    [1, 2, 3],                  # list\n",
    "    (4, 5, 6),                  # tuple\n",
    "    {\"key\": \"value\"},           # dict\n",
    "    3.14,                       # s·ªë th·ª±c\n",
    "    True                        # boolean\n",
    "]\n",
    "\n",
    "rdd5 = sc.parallelize(mixed_data, 4)\n",
    "print(f\"Detaied partitions: {rdd5.glom().collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586fc7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== f) M·ªôt text file ===\n",
      "Content of README file:\n",
      "# Welcome to Apache Spark\n",
      "Apache Spark is a powerful big data processing engine.\n",
      "This lab teaches Spark RDD operations.\n",
      "Spark provides distributed computing capabilities.\n",
      "Big Data Analytics with Spark framework.\n",
      "Python is great for data science.\n",
      "\n",
      "read file by Spark:\n",
      " Line 1: # Welcome to Apache Spark\n",
      " Line 2: Apache Spark is a powerful big data processing engine.\n",
      " Line 3: This lab teaches Spark RDD operations.\n",
      " Line 4: Spark provides distributed computing capabilities.\n",
      " Line 5: Big Data Analytics with Spark framework.\n",
      " Line 6: Python is great for data science.\n",
      "Detaied partitions: [['# Welcome to Apache Spark', 'Apache Spark is a powerful big data processing engine.', 'This lab teaches Spark RDD operations.'], ['Spark provides distributed computing capabilities.'], ['Big Data Analytics with Spark framework.', 'Python is great for data science.']]\n"
     ]
    }
   ],
   "source": [
    "# ===== B√ÄI 1f: TEXT FILE =====\n",
    "print(\"=== f) M·ªôt text file ===\")\n",
    "\n",
    "import os\n",
    "data_path = \"../../data/README.md\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(\"Content of README file:\")\n",
    "print(content)\n",
    "\n",
    "rdd6 = sc.textFile(data_path, 3)\n",
    "print(\"read file by Spark:\")\n",
    "lines = rdd6.collect()\n",
    "for i, line in enumerate(lines, 1):\n",
    "    print(f\" Line {i}: {line}\")\n",
    "\n",
    "print(f\"Detaied partitions: {rdd6.glom().collect()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
